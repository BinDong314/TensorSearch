{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9849b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def concatenate_hdf5_files(directory_path, dataset_path):\n",
    "    # Initialize an empty list to store the data from each HDF5 file\n",
    "    data_list = []\n",
    "\n",
    "    # Check if directory_path is a file\n",
    "    if os.path.isfile(directory_path):\n",
    "        with h5py.File(directory_path, 'r') as h5file:\n",
    "            # Assuming your data is stored in a dataset named 'data' within the HDF5 file\n",
    "            data = h5file[dataset_path][:]\n",
    "            data_list.append(data)\n",
    "    else:\n",
    "        # Iterate through all files in the directory\n",
    "        for filename in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with h5py.File(file_path, 'r') as h5file:\n",
    "                # Assuming your data is stored in a dataset named 'data' within the HDF5 file\n",
    "                data = h5file[dataset_path][:]\n",
    "                data_list.append(data)\n",
    "\n",
    "    # Concatenate the data from all files into a single NumPy array\n",
    "    if data_list:\n",
    "        #print(len(data_list))\n",
    "        # Determine the concatenation axis based on the dimensionality of the data\n",
    "        data_dimensionality = data_list[0].ndim\n",
    "        if data_dimensionality == 1:\n",
    "            concatenated_data = np.stack(data_list, axis=0)\n",
    "        else:\n",
    "            concatenated_data = np.concatenate(data_list, axis=data_dimensionality - 1)\n",
    "\n",
    "        return concatenated_data\n",
    "    else:\n",
    "        return None  # Handle the case where there are no HDF5 files in the directory\n",
    "\n",
    "def load_data(db_data_file_dir, db_data_dataset,  pattern_data_file_dir, pattern_data_dataset):\n",
    "    db_data = concatenate_hdf5_files(db_data_file_dir, db_data_dataset)\n",
    "    if db_data is not None:\n",
    "        print(\"Concatenated data shape:\", db_data.shape)\n",
    "    \n",
    "    db_pattern = concatenate_hdf5_files(pattern_data_file_dir, pattern_data_dataset)\n",
    "    if db_pattern is not None:\n",
    "        print(\"Concatenated data shape:\", db_pattern.shape)\n",
    "    return db_data, db_pattern \n",
    "\n",
    "#test code\n",
    "# db_data, db_pattern = load_data(db_data_file_dir = \"/Users/dbin/work/TensorSearch/db-data-1d\", db_data_dataset = \"/testg/testd\",  pattern_data_file_dir = \"/Users/dbin/work/TensorSearch/db-pattern-1d\", pattern_data_dataset =  \"/testg/testd\");\n",
    "# print(db_data)\n",
    "# print(db_pattern)\n",
    "\n",
    "# db_data, db_pattern = load_data(db_data_file_dir = \"./deep-image-96-angular.hdf5\", db_data_dataset = \"/train\",  pattern_data_file_dir = \"./deep-image-96-angular.hdf5\", pattern_data_dataset =  \"/test\");\n",
    "# print(db_data)\n",
    "# print(db_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f06541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Create connection...\n",
      "\n",
      "List connections:\n",
      "[('default', <pymilvus.client.grpc_handler.GrpcHandler object at 0x11e82bbb0>)]\n",
      "\n",
      "collection created: demo\n",
      "\n",
      "list collections:\n",
      "['demo']\n",
      "Concatenated data shape: (9990000, 96)\n",
      "Concatenated data shape: (10000, 96)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" example.py based from pymilvus\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "from milvus import default_server\n",
    "\n",
    "\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    FieldSchema, CollectionSchema, DataType,\n",
    "    Collection,\n",
    "    utility\n",
    ")\n",
    "\n",
    "# This example shows how to:\n",
    "#   1. connect to Milvus server\n",
    "#   2. create a collection\n",
    "#   3. insert entities\n",
    "#   4. create index\n",
    "#   5. search\n",
    "\n",
    "# Optional, if you want store all related data to specific location\n",
    "# default it wil using %APPDATA%/milvus-io/milvus-server\n",
    "default_server.set_base_dir('test_milvus')\n",
    "default_server.stop()\n",
    "# Optional, if you want cleanup previous data\n",
    "default_server.cleanup()\n",
    "# star you milvus server\n",
    "default_server.start()\n",
    "\n",
    "_HOST = '127.0.0.1'\n",
    "# The port may be changed, by default it's 19530\n",
    "_PORT = default_server.listen_port\n",
    "\n",
    "# Const names\n",
    "_COLLECTION_NAME = 'demo'\n",
    "_ID_FIELD_NAME = 'id_field'\n",
    "_VECTOR_FIELD_NAME = 'float_vector_field'\n",
    "\n",
    "# Vector parameters\n",
    "_DIM = 96\n",
    "#_INDEX_FILE_SIZE = 32  # max file size of stored index\n",
    "\n",
    "# Index parameters\n",
    "_METRIC_TYPE = 'IP'\n",
    "_INDEX_TYPE = 'FLAT'\n",
    "_NLIST = 1024\n",
    "_NPROBE = 16\n",
    "_TOPK = 10\n",
    "\n",
    "\n",
    "# Create a Milvus connection\n",
    "def create_connection():\n",
    "    print(f\"\\nCreate connection...\")\n",
    "    connections.connect(host=_HOST, port=_PORT)\n",
    "    print(f\"\\nList connections:\")\n",
    "    print(connections.list_connections())\n",
    "\n",
    "\n",
    "# Create a collection named 'demo'\n",
    "def create_collection(name, id_field, vector_field):\n",
    "    field1 = FieldSchema(name=id_field, dtype=DataType.INT64, description=\"int64\", is_primary=True)\n",
    "    field2 = FieldSchema(name=vector_field, dtype=DataType.FLOAT_VECTOR, description=\"float vector\", dim=_DIM,\n",
    "                         is_primary=False)\n",
    "    schema = CollectionSchema(fields=[field1, field2], description=\"collection description\")\n",
    "    collection = Collection(name=name, data=None, schema=schema, properties={\"collection.ttl.seconds\": 15})\n",
    "    print(\"\\ncollection created:\", name)\n",
    "    return collection\n",
    "\n",
    "\n",
    "def has_collection(name):\n",
    "    return utility.has_collection(name)\n",
    "\n",
    "\n",
    "# Drop a collection in Milvus\n",
    "def drop_collection(name):\n",
    "    collection = Collection(name)\n",
    "    collection.drop()\n",
    "    print(\"\\nDrop collection: {}\".format(name))\n",
    "\n",
    "\n",
    "# List all collections in Milvus\n",
    "def list_collections():\n",
    "    print(\"\\nlist collections:\")\n",
    "    print(utility.list_collections())\n",
    "\n",
    "\n",
    "def insert(collection, db_data):\n",
    "    data = [\n",
    "        [i for i in range(db_data.shape[0])],\n",
    "        db_data.tolist(),\n",
    "    ]\n",
    "    #print(data)\n",
    "    collection.insert(data)\n",
    "    \n",
    "\n",
    "\n",
    "def get_entity_num(collection):\n",
    "    print(\"\\nThe number of entity:\")\n",
    "    print(collection.num_entities)\n",
    "\n",
    "\n",
    "def create_index(collection, filed_name):\n",
    "    index_param = {\n",
    "        \"index_type\": _INDEX_TYPE,\n",
    "        \"params\": {\"nlist\": _NLIST},\n",
    "        \"metric_type\": _METRIC_TYPE}\n",
    "    collection.create_index(filed_name, index_param)\n",
    "    print(\"\\nCreated index:\\n{}\".format(collection.index().params))\n",
    "\n",
    "\n",
    "def drop_index(collection):\n",
    "    collection.drop_index()\n",
    "    print(\"\\nDrop index sucessfully\")\n",
    "\n",
    "\n",
    "def load_collection(collection):\n",
    "    collection.load()\n",
    "\n",
    "\n",
    "def release_collection(collection):\n",
    "    collection.release()\n",
    "\n",
    "\n",
    "def search(collection, vector_field, id_field, search_vectors):\n",
    "    search_param = {\n",
    "        \"data\": search_vectors,\n",
    "        \"anns_field\": vector_field,\n",
    "        \"param\": {\"metric_type\": _METRIC_TYPE, \"params\": {\"nprobe\": _NPROBE}},\n",
    "        \"limit\": _TOPK,\n",
    "        \"expr\": \"id_field >= 0\"}\n",
    "    results = collection.search(**search_param)\n",
    "    for i, result in enumerate(results):\n",
    "        print(\"\\nSearch result for {}th vector: \".format(i))\n",
    "        for j, res in enumerate(result):\n",
    "            print(\"Top {}: {}\".format(j, res))\n",
    "\n",
    "\n",
    "def set_properties(collection):\n",
    "    collection.set_properties(properties={\"collection.ttl.seconds\": 1800})\n",
    "\n",
    "\n",
    "def main():\n",
    "    # create a connection\n",
    "    create_connection()\n",
    "\n",
    "    # drop collection if the collection exists\n",
    "    if has_collection(_COLLECTION_NAME):\n",
    "        drop_collection(_COLLECTION_NAME)\n",
    "\n",
    "    # create collection\n",
    "    collection = create_collection(_COLLECTION_NAME, _ID_FIELD_NAME, _VECTOR_FIELD_NAME)\n",
    "\n",
    "    # alter ttl properties of collection level\n",
    "    set_properties(collection)\n",
    "\n",
    "    # show collections\n",
    "    list_collections()\n",
    "   \n",
    "    db_data, db_pattern = load_data(db_data_file_dir = \"./deep-image-96-angular.hdf5\", db_data_dataset = \"/train\",  pattern_data_file_dir = \"./deep-image-96-angular.hdf5\", pattern_data_dataset =  \"/test\");\n",
    "    #db_data = db_data[0:10, :]\n",
    "    #db_pattern = db_pattern[0:10, :]\n",
    "\n",
    "    # insert 10000 vectors with 128 dimension\n",
    "    #df = pd.DataFrame(db_data)\n",
    "    insert(collection, db_data)\n",
    "    # print(vectors)\n",
    "    collection.flush()\n",
    "    # get the number of entities\n",
    "    get_entity_num(collection)\n",
    "\n",
    "    # create index\n",
    "    create_index(collection, _VECTOR_FIELD_NAME)\n",
    "\n",
    "    # load data to memory\n",
    "    load_collection(collection)\n",
    "\n",
    "    # search\n",
    "    search(collection, _VECTOR_FIELD_NAME, _ID_FIELD_NAME, db_pattern)\n",
    "\n",
    "    # release memory\n",
    "    release_collection(collection)\n",
    "\n",
    "    # drop collection index\n",
    "    # drop_index(collection)\n",
    "\n",
    "    # drop collection\n",
    "    drop_collection(_COLLECTION_NAME)\n",
    "\n",
    "main()\n",
    "default_server.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bfc632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[[0, 1], [[0.3515893227098098, 0.014051220768119932, 0.5188800444922963, 0.24591186833394352], [0.19807924413544042, 0.6490144610433105, 0.0538598294335535, 0.7194652749658635]]]\n"
     ]
    }
   ],
   "source": [
    "def insert(num, dim):\n",
    "    data = [\n",
    "        [i for i in range(num)],\n",
    "        [[random.random() for _ in range(dim)] for _ in range(num)],\n",
    "    ]\n",
    "    print(type(data))\n",
    "    print(data)\n",
    "\n",
    "    \n",
    "insert(2, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c72e41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of V1 and V2: 714.09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "V1 = [7.5, 49.5, 73.5, 58.5]\n",
    "V2 = [1.81, 2.81, 3.81, 4.81]\n",
    "\n",
    "dot_product = np.dot(V1, V2)\n",
    "\n",
    "# Alternatively, you can use np.inner(V1, V2) to achieve the same result\n",
    "# dot_product = np.inner(V1, V2)\n",
    "\n",
    "print(\"Dot product of V1 and V2:\", dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
